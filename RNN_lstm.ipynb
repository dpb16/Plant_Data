{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05598082-a82f-437d-ac73-0c295a63567c",
   "metadata": {},
   "source": [
    "Design RNN or its variant including LSTM or GRU a) Select a suitable time series dataset.      \n",
    "Example – predict sentiments based on product reviews b) Apply for prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea21050e-65d5-46c9-b485-f5d09f9b9fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e6fdd56-4f49-442f-9d7e-81663bd20c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n            Explanation:\\n            vocab_size=5000: Restrict vocabulary to the 5,000 most frequent words (reduces noise from rare words).\\n            imdb.load_data(): Loads the IMDB dataset preprocessed into integer sequences.\\n            x_train/x_test: Lists of reviews, where each word is replaced by its integer index.\\n            y_train/y_test: Labels (0 or 1).\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explanation:\n",
    "\"\"\" \n",
    "            Sequential: A linear stack of layers to build the model.\n",
    "            Embedding: Converts integer-encoded words into dense vectors (e.g., \"cat\" → [0.2, -0.5, ...]).\n",
    "            LSTM: Layer to process sequential data with memory cells and gates.\n",
    "            Dense: Fully connected layer for classification.\n",
    "            imdb: Preloaded dataset of movie reviews labeled as positive (1) or negative (0).\n",
    "            sequence: Utilities for padding sequences to a fixed length. \n",
    "\"\"\"\n",
    "\n",
    "# Load dataset\n",
    "vocab_size = 5000  # Use top 5,000 frequent words\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "\"\"\"\n",
    "            Explanation:\n",
    "            vocab_size=5000: Restrict vocabulary to the 5,000 most frequent words (reduces noise from rare words).\n",
    "            imdb.load_data(): Loads the IMDB dataset preprocessed into integer sequences.\n",
    "            x_train/x_test: Lists of reviews, where each word is replaced by its integer index.\n",
    "            y_train/y_test: Labels (0 or 1).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb75e5d4-4af3-4891-a6ec-97da48f22699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OJAS VIJAY AMBEKAR\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n            Step 1: Embedding Layer:\\n            vocab_size: 5,000 unique words.\\n            32: Embedding dimension (each word is a 32-dimensional vector).\\n            input_length=max_words: Each input sequence has 400 words.\\n            Purpose: Converts sparse integer-encoded words into dense vectors that capture semantic meaning (e.g., \"good\" and \"great\" are closer in vector space).\\n\\n            Step 2: LSTM Layer:\\n            128: Number of LSTM units (dimensionality of the hidden state).\\n            activation=\\'tanh\\': Hyperbolic tangent activation for gate updates.\\n            return_sequences=False: Return only the final output (not all timesteps).\\n            Purpose: Processes the sequence word-by-word, updating its hidden state to capture context.\\n\\n            Step 3: Dense Layer:\\n            1: Single neuron for binary classification (positive/negative).\\n            activation=\\'sigmoid\\': Squashes output to [0, 1] (probability of positive sentiment).\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad sequences to fixed length (400 words)\n",
    "max_words = 400\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_words)\n",
    "\"\"\"\n",
    "            Explanation:\n",
    "            max_words=400: Truncate/pad all reviews to 400 words.\n",
    "            Shorter reviews are padded with zeros (e.g., [0, 0, ..., 12, 42]).\n",
    "            Longer reviews are truncated to 400 words.\n",
    "            Why? Neural networks require fixed-length inputs for batch processing.\n",
    "\"\"\"\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential(name=\"LSTM_Sentiment_Analysis\")\n",
    "model.add(Embedding(vocab_size, 32, input_length=max_words))  # Convert word indices to 32D vectors\n",
    "model.add(LSTM(128, activation='tanh', return_sequences=False))  # 128 LSTM units\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "\"\"\"\n",
    "            Step 1: Embedding Layer:\n",
    "            vocab_size: 5,000 unique words.\n",
    "            32: Embedding dimension (each word is a 32-dimensional vector).\n",
    "            input_length=max_words: Each input sequence has 400 words.\n",
    "            Purpose: Converts sparse integer-encoded words into dense vectors that capture semantic meaning (e.g., \"good\" and \"great\" are closer in vector space).\n",
    "\n",
    "            Step 2: LSTM Layer:\n",
    "            128: Number of LSTM units (dimensionality of the hidden state).\n",
    "            activation='tanh': Hyperbolic tangent activation for gate updates.\n",
    "            return_sequences=False: Return only the final output (not all timesteps).\n",
    "            Purpose: Processes the sequence word-by-word, updating its hidden state to capture context.\n",
    "\n",
    "            Step 3: Dense Layer:\n",
    "            1: Single neuron for binary classification (positive/negative).\n",
    "            activation='sigmoid': Squashes output to [0, 1] (probability of positive sentiment).\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336b6e03-52ea-4525-83b8-a62472844ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 607ms/step - accuracy: 0.6384 - loss: 0.6120 - val_accuracy: 0.8270 - val_loss: 0.3980\n",
      "Epoch 2/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m468s\u001b[0m 1s/step - accuracy: 0.8688 - loss: 0.3252 - val_accuracy: 0.8732 - val_loss: 0.3075\n",
      "Epoch 3/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 1s/step - accuracy: 0.9009 - loss: 0.2542 - val_accuracy: 0.8552 - val_loss: 0.3424\n",
      "Epoch 4/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 556ms/step - accuracy: 0.8960 - loss: 0.2588 - val_accuracy: 0.8464 - val_loss: 0.3528\n",
      "Epoch 5/5\n",
      "\u001b[1m 40/313\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:00\u001b[0m 441ms/step - accuracy: 0.9272 - loss: 0.2151"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\"\"\"\n",
    "            Explanation:\n",
    "            loss='binary_crossentropy': Standard loss for binary classification.\n",
    "            optimizer='adam': Adaptive learning rate optimizer (efficient for RNNs).\n",
    "            metrics=['accuracy']: Track accuracy during training.\n",
    "\"\"\"\n",
    "\n",
    "# Train model\n",
    "history = model.fit(x_train, y_train, \n",
    "                   batch_size=64, \n",
    "                   epochs=5, \n",
    "                   validation_split=0.2)\n",
    "\"\"\"\n",
    "            Explanation:\n",
    "            batch_size=64: Update weights after every 64 samples (balance speed/memory).\n",
    "            epochs=5: Train for 5 full passes over the training data.\n",
    "            validation_split=0.2: Use 20% of training data for validation (monitor overfitting).\n",
    "            Output: Training logs show loss/accuracy for training and validation sets.\n",
    "\"\"\"\n",
    "\n",
    "# Evaluate on test data\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\"\"\"\n",
    "            Explanation:\n",
    "            evaluate(): Computes loss and accuracy on unseen test data.\n",
    "            test_acc: Accuracy reflects how well the model generalizes to new reviews.\n",
    "            Typical Output: ~80-88% accuracy depending on hyperparameters.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47c6830-483d-4cec-8e0b-5ac5d783fd8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
